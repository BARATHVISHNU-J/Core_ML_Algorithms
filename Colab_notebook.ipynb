{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "View Readme file for the colab link"
      ],
      "metadata": {
        "id": "0cGPhDx_IACS"
      },
      "id": "0cGPhDx_IACS"
    },
    {
      "cell_type": "markdown",
      "id": "abe5a67f",
      "metadata": {
        "id": "abe5a67f"
      },
      "source": [
        "# ML ALGORITHMS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "# ML ALGORITHMS\n",
        "\n"
      ],
      "metadata": {
        "id": "Mzg9rUNQz4sx"
      },
      "id": "Mzg9rUNQz4sx"
    },
    {
      "cell_type": "markdown",
      "id": "7ac644dc",
      "metadata": {
        "id": "7ac644dc"
      },
      "source": [
        "### LINEAR REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.datasets import load_diabetes"
      ],
      "metadata": {
        "id": "Wldu-55u7QP5"
      },
      "id": "Wldu-55u7QP5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e85be75c",
      "metadata": {
        "id": "e85be75c"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ========== LINEAR REGRESSION: House Price Prediction ==========\n",
        "print(\"\\n LINEAR REGRESSION: House Price Prediction\\n\")\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "    \"Size (sqft)\": [500, 700, 1000, 1200, 1500, 1800, 2000, 2300],\n",
        "    \"Price (Lakh â‚¹)\": [20, 28, 40, 48, 60, 72, 80, 92]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "X = df[[\"Size (sqft)\"]]\n",
        "y = df[\"Price (Lakh â‚¹)\"]\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "#  User input\n",
        "user_size = int(input(\"Enter house size in sqft: \"))\n",
        "predicted_price = model.predict([[user_size]])[0]\n",
        "print(f\"âœ… Estimated price: â‚¹{predicted_price:.2f} Lakhs\")\n",
        "\n",
        "#  Plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(X, y, color='blue', label='Training Data')\n",
        "plt.plot(X, model.predict(X), color='red', label='Regression Line')\n",
        "plt.scatter(user_size, predicted_price, color='green', label='Your Input', s=100)\n",
        "plt.xlabel(\"Size (sqft)\")\n",
        "plt.ylabel(\"Price (Lakh â‚¹)\")\n",
        "plt.title(\"Linear Regression: House Size vs Price\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3258d5b7",
      "metadata": {
        "id": "3258d5b7"
      },
      "source": [
        "### LOGISTIC REGRESSION\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "# ========== LOGISTIC REGRESSION: Diabetes Prediction ==========\n",
        "print(\"\\n LOGISTIC REGRESSION: Diabetes Risk Prediction\\n\")\n",
        "\n",
        "# Load dataset and convert to binary classification\n",
        "diabetes = load_diabetes(as_frame=True)\n",
        "df2 = diabetes.frame\n",
        "df2['Outcome'] = (df2['target'] > 140).astype(int)\n",
        "X2 = df2[['bmi', 'bp']]\n",
        "y2 = df2['Outcome']\n",
        "\n",
        "# Train logistic model\n",
        "log_model = LogisticRegression()\n",
        "log_model.fit(X2, y2)\n",
        "\n",
        "#  User input\n",
        "user_bmi = float(input(\"Enter normalized BMI (between -0.1 and 0.2): \"))\n",
        "user_bp = float(input(\"Enter normalized Blood Pressure (between 0.01 and 0.2): \"))\n",
        "\n",
        "log_pred = log_model.predict([[user_bmi, user_bp]])[0]\n",
        "log_prob = log_model.predict_proba([[user_bmi, user_bp]])[0]\n",
        "\n",
        "status = \"Diabetic\" if log_pred == 1 else \"Non-Diabetic\"\n",
        "print(f\" Prediction: {status}\")\n",
        "print(f\" Probability: {log_prob[1]*100:.2f}% chance of being diabetic\")\n",
        "\n",
        "#  Plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(X2['bmi'], X2['bp'], c=y2, cmap='bwr', edgecolors='k', alpha=0.6)\n",
        "plt.scatter(user_bmi, user_bp, color='black', s=100, label='Your Input')\n",
        "plt.xlabel(\"BMI (normalized)\")\n",
        "plt.ylabel(\"Blood Pressure (normalized)\")\n",
        "plt.title(\"Logistic Regression: Diabetes Risk\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HSQLrRX_5si7"
      },
      "id": "HSQLrRX_5si7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "eb360af5",
      "metadata": {
        "id": "eb360af5"
      },
      "source": [
        "### DECISION TREE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#--Decision Trees-- Titanic Survival Predictions\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "66CrFOfe6CLz"
      },
      "id": "66CrFOfe6CLz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "# Show initial data\n",
        "df[['survived', 'pclass', 'sex', 'age', 'fare']].head()"
      ],
      "metadata": {
        "id": "tAEidv9VAqsD"
      },
      "id": "tAEidv9VAqsD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only useful columns\n",
        "df = df[['survived', 'pclass', 'sex', 'age', 'fare']]\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "# Encode 'sex' as 0 (male) and 1 (female)\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "# Split into features and label\n",
        "X = df[['pclass', 'sex', 'age', 'fare']]\n",
        "y = df['survived']\n"
      ],
      "metadata": {
        "id": "lqvXtGc6ArBh"
      },
      "id": "lqvXtGc6ArBh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Train decision tree\n",
        "model = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "HTz7NxQfArTq"
      },
      "id": "HTz7NxQfArTq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Not Survived', 'Survived']))\n"
      ],
      "metadata": {
        "id": "_EDKz_L5AriC"
      },
      "id": "_EDKz_L5AriC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot\n",
        "plt.figure(figsize=(14, 8))\n",
        "plot_tree(model, feature_names=X.columns, class_names=['Not Survived', 'Survived'], filled=True)\n",
        "plt.title(\"Titanic Decision Tree\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gAQ4H5wwArxZ"
      },
      "id": "gAQ4H5wwArxZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format: [pclass, sex, age, fare]\n",
        "# Example: 3rd class, female, 22 yrs, fare $155\n",
        "sample = [[1, 1, 22, 155]]\n",
        "\n",
        "prediction = model.predict(sample)\n",
        "print(\"Prediction:\", \"Survived\" if prediction[0] == 1 else \"Not Survived\")"
      ],
      "metadata": {
        "id": "EgxN3aatA1b5"
      },
      "id": "EgxN3aatA1b5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show Feature Importances\n",
        "feature_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "feature_importances.sort_values(ascending=False).plot(kind='barh', title='Feature Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tl2mb_1eA1uh"
      },
      "id": "Tl2mb_1eA1uh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Gradient Boosting (XG Boost)\n",
        "# ðŸ“¦ Import libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# ðŸ“¥ Load and preprocess Titanic dataset\n",
        "df = sns.load_dataset(\"titanic\")\n",
        "\n",
        "# Select useful columns\n",
        "df = df[[\"survived\", \"pclass\", \"sex\", \"age\", \"fare\", \"embarked\"]]\n",
        "df.dropna(inplace=True)  # Drop rows with missing values\n",
        "\n",
        "# Encode categorical features\n",
        "df[\"sex\"] = LabelEncoder().fit_transform(df[\"sex\"])\n",
        "df[\"embarked\"] = LabelEncoder().fit_transform(df[\"embarked\"])\n",
        "\n",
        "# Split into features and target\n",
        "X = df.drop(\"survived\", axis=1)\n",
        "y = df[\"survived\"]\n",
        "\n",
        "# ðŸ”€ Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# ðŸš€ Train XGBoost classifier\n",
        "model = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# ðŸ“Š Evaluate model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"ðŸŽ¯ Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nðŸ“‹ Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# ðŸ” Feature Importance\n",
        "xgb.plot_importance(model)\n",
        "plt.title(\"Feature Importance - Titanic\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h-ZHaOE9cMui"
      },
      "id": "h-ZHaOE9cMui",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "14c5f731",
      "metadata": {
        "id": "14c5f731"
      },
      "source": [
        "### K NEAREST NEIGHBORS(KNN)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor, NearestNeighbors,KNeighborsClassifier\n",
        "from sklearn.metrics import r2_score,accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "LYyk1v7fHLZK"
      },
      "id": "LYyk1v7fHLZK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "006ed200",
      "metadata": {
        "id": "006ed200"
      },
      "source": [
        "#### KNN REGRESSOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac327173",
      "metadata": {
        "id": "ac327173"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Load the sample regression dataset from CSV\n",
        "sample_df = pd.read_csv(\"https://raw.githubusercontent.com/BARATHVISHNU-J/Core_ML_Algorithms/main/sample_regression.csv\")\n",
        "print(sample_df.head())  # Display the first few rows of the dataset\n",
        "\n",
        "# Features: feature1 and feature2, Target: target\n",
        "X = sample_df[['feature1', 'feature2']].values\n",
        "y = sample_df['target'].values\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=22)\n",
        "\n",
        "# Create a KNN regressor object with k=3\n",
        "knn = KNeighborsRegressor(n_neighbors=3)\n",
        "\n",
        "# Train the KNN regressor using the training data\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict the target for the test set\n",
        "y_pred = knn.predict(X_test)\n",
        "# Predict the target for the train set\n",
        "y_train_pred = knn.predict(X_train)\n",
        "\n",
        "train_preds = knn.predict(X_train)\n",
        "test_preds = knn.predict(X_test)\n",
        "print(\"\\nPredicted target values:\")\n",
        "print(\"Train set predictions:\")\n",
        "print(train_preds)\n",
        "print(\"Test set predictions:\")\n",
        "print(test_preds)\n",
        "# Calculate and print the mean squared error (mse) and R^2 score of the regressor on the test set\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R^2 Score (Accuracy):\", (round(r2, 3))*100, \"%\")\n",
        "\n",
        "# Visualization: plot feature1 vs feature2 for train and test sets (no user input, solid colors, with target values)\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], color='blue', label='Train', alpha=0.6, edgecolor='k')\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], color='red', label='Test', alpha=0.8, edgecolor='k')\n",
        "# Annotate train points with their target values\n",
        "for i in range(len(X_train)):\n",
        "    plt.text(X_train[i, 0], X_train[i, 1], f'{y_train[i]:.2f}', fontsize=8, color='navy', ha='right', va='bottom')\n",
        "# Annotate test points with their target values\n",
        "for i in range(len(X_test)):\n",
        "    plt.text(X_test[i, 0], X_test[i, 1], f'{y_test[i]:.2f}', fontsize=8, color='darkred', ha='left', va='top')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('KNN Regression: Feature Space (Train vs Test)')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### USER INPUT\n"
      ],
      "metadata": {
        "id": "gwJx_XiPG5PQ"
      },
      "id": "gwJx_XiPG5PQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- User input and prediction ---\n",
        "user_point = None\n",
        "mean_pred = None\n",
        "median_pred = None\n",
        "f1 = float(input(\"Enter value for feature1: \"))\n",
        "f2 = float(input(\"Enter value for feature2: \"))\n",
        "user_point = np.array([[f1, f2]])\n",
        "# Predict using mean (default KNN)\n",
        "mean_pred = knn.predict(user_point)[0]\n",
        "# Predict using median of k nearest neighbors\n",
        "nn = NearestNeighbors(n_neighbors=3)\n",
        "nn.fit(X_train)\n",
        "distances, indices = nn.kneighbors(user_point)\n",
        "neighbor_targets = y_train[indices[0]]\n",
        "median_pred = np.median(neighbor_targets)\n",
        "print(f\"Predicted target (mean of neighbors): {mean_pred:.2f}\")\n",
        "print(f\"Predicted target (median of neighbors): {median_pred:.2f}\")\n",
        "\n",
        "# Visualization: plot feature1 vs feature2 for train, test, and user input (solid colors, with target values)\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], color='blue', label='Train', alpha=0.6, edgecolor='k')\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], color='red', label='Test', alpha=0.8, edgecolor='k')\n",
        "plt.scatter(f1, f2, color='magenta', s=200, marker='*', label=f'User Input (pred: {mean_pred:.2f})')\n",
        "# Annotate train points with their target values\n",
        "for i in range(len(X_train)):\n",
        "    plt.text(X_train[i, 0], X_train[i, 1], f'{y_train[i]:.2f}', fontsize=8, color='navy', ha='right', va='bottom')\n",
        "# Annotate test points with their target values\n",
        "for i in range(len(X_test)):\n",
        "    plt.text(X_test[i, 0], X_test[i, 1], f'{y_test[i]:.2f}', fontsize=8, color='darkred', ha='left', va='top')\n",
        "# Annotate user input with its predicted value\n",
        "plt.text(f1, f2, f'{mean_pred:.2f}', fontsize=10, color='magenta', ha='center', va='bottom', fontweight='bold')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('KNN Regression: Feature Space (Train, Test, User Input)')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q3rGg7heG7-z"
      },
      "id": "q3rGg7heG7-z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c95d2381",
      "metadata": {
        "id": "c95d2381"
      },
      "source": [
        "#### KNN CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdc59b1a",
      "metadata": {
        "id": "bdc59b1a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor, NearestNeighbors,KNeighborsClassifier\n",
        "from sklearn.metrics import r2_score,accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the movies dataset from CSV\n",
        "movies_df = pd.read_csv(\"https://raw.githubusercontent.com/BARATHVISHNU-J/Core_ML_Algorithms/main/movies.csv\")\n",
        "\n",
        "# Encode movie genres as integers for KNN classification\n",
        "le_genre = LabelEncoder()\n",
        "movies_df['genre_encoded'] = le_genre.fit_transform(movies_df['genre'])\n",
        "\n",
        "print(movies_df)\n",
        "# Features: imdb_rating and duration, Target: genre_encoded\n",
        "X = movies_df[['imdb_rating', 'duration']].values\n",
        "y = movies_df['genre_encoded'].values\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "# Split the dataset into training and testing sets (indices for original X)\n",
        "train_idx, test_idx = train_test_split(np.arange(len(X)), test_size=0.4, random_state=4)\n",
        "X_train, X_test, y_train, y_test = X_scaled[train_idx], X_scaled[test_idx], y[train_idx], y[test_idx]\n",
        "\n",
        "# Use original (unscaled) data for plotting\n",
        "X_train_orig, X_test_orig = X[train_idx], X[test_idx]\n",
        "\n",
        "# Find the best k for KNN\n",
        "best_k = 1\n",
        "best_acc = 0\n",
        "for k in range(1, min(11, len(X_train))):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        best_k = k\n",
        "print(f\"Best k: {best_k} with accuracy: {round(best_acc*100, 2)}%\")\n",
        "\n",
        "# Use the best k for final model\n",
        "knn = KNeighborsClassifier(n_neighbors=best_k)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "# Calculate and print the accuracy of the classifier on the test set\n",
        "print(\"Final Accuracy:\", round(accuracy_score(y_test, y_pred)*100, 2), \"%\")\n",
        "\n",
        "# Visualization: plot IMDb rating vs duration, color by genre\n",
        "# Define a list of distinct colors for each genre\n",
        "unique_genres = len(le_genre.classes_)\n",
        "color_list = plt.cm.tab10(np.linspace(0, 1, unique_genres))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "# Plot all movies with genre-specific colors\n",
        "for idx, genre in enumerate(le_genre.classes_):\n",
        "    mask = movies_df['genre'] == genre\n",
        "    plt.scatter(movies_df.loc[mask, 'imdb_rating'], movies_df.loc[mask, 'duration'],\n",
        "                color=color_list[idx], label=genre, alpha=0.7, edgecolor='k')\n",
        "# Highlight test set with a different color (e.g., yellow)\n",
        "plt.scatter(X_test_orig[:, 0], X_test_orig[:, 1], c='yellow', edgecolor='black', s=120, label='Test Movies')\n",
        "plt.xlabel('IMDb Rating')\n",
        "plt.ylabel('Duration (min)')\n",
        "plt.title('KNN Classification: Predicting Movie Genre from IMDb Rating and Duration')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Print test set details: IMDb rating, duration, true genre, predicted genre, and movie name\n",
        "print(\"\\nTest Set Details:\")\n",
        "test_indices = X_test.shape[0]\n",
        "for i in range(test_indices):\n",
        "    # Use original (unscaled) test data for matching\n",
        "    imdb_val = X_test_orig[i, 0]\n",
        "    duration_val = X_test_orig[i, 1]\n",
        "    mask = (movies_df['imdb_rating'] == imdb_val) & (movies_df['duration'] == duration_val)\n",
        "    movie_name = movies_df[mask]['name'].values[0] if not movies_df[mask].empty else 'Unknown'\n",
        "    true_genre = le_genre.inverse_transform([y_test[i]])[0]\n",
        "    pred_genre = le_genre.inverse_transform([y_pred[i]])[0]\n",
        "    print(f\"Movie: {movie_name}, IMDb: {imdb_val}, Duration: {duration_val}, True Genre: {true_genre}, Predicted Genre: {pred_genre}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### user input"
      ],
      "metadata": {
        "id": "yV1WfkIuM0yy"
      },
      "id": "yV1WfkIuM0yy"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- User input and prediction ---\n",
        "try:\n",
        "    user_rating = float(input(\"Enter IMDb rating: \"))\n",
        "    user_duration = float(input(\"Enter duration (min): \"))\n",
        "    user_point = scaler.transform(np.array([[user_rating, user_duration]]))\n",
        "    user_pred = knn.predict(user_point)[0]\n",
        "    user_genre = le_genre.inverse_transform([user_pred])[0]\n",
        "    print(f\"Predicted genre for input (IMDb: {user_rating}, Duration: {user_duration}): {user_genre}\")\n",
        "    # Plot training data and user input\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    for genre in le_genre.classes_:\n",
        "        genre_code = le_genre.transform([genre])[0]\n",
        "        mask = y_train == genre_code\n",
        "        if np.any(mask):\n",
        "            plt.scatter(X_train_orig[mask, 0], X_train_orig[mask, 1], color=color_list[genre_code], label=genre, alpha=0.7, edgecolor='k')\n",
        "        else:\n",
        "            plt.scatter([], [], color=color_list[genre_code], label=genre)\n",
        "    plt.scatter(user_rating, user_duration, color='yellow', edgecolor='black', s=200, marker='*', label='User Input')\n",
        "    plt.xlabel('IMDb Rating')\n",
        "    plt.ylabel('Duration (min)')\n",
        "    plt.title('Training Samples and User Input')\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"User input prediction skipped: {e}\")"
      ],
      "metadata": {
        "id": "gHWCHaweM2wI"
      },
      "id": "gHWCHaweM2wI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0e7bdfe7",
      "metadata": {
        "id": "0e7bdfe7"
      },
      "source": [
        "### SUPPORT VECTOR MACHINE (SVM) CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "oVsb8HLlN4jb"
      },
      "id": "oVsb8HLlN4jb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xO39gC-wzjL6"
      },
      "id": "xO39gC-wzjL6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "271a7040",
      "metadata": {
        "id": "271a7040"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load simple dataset\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/BARATHVISHNU-J/Core_ML_Algorithms/main/dataset.csv\")\n",
        "X = df[['feature1', 'feature2']].values\n",
        "y = df['label'].values\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = svm.predict(X_test)\n",
        "print('Accuracy:', round(accuracy_score(y_test, y_pred),2)*100, '%')\n",
        "\n",
        "# Plot data and decision boundary\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(X_train[y_train==0,0], X_train[y_train==0,1], color='blue', label='Class 0 (train)')\n",
        "plt.scatter(X_train[y_train==1,0], X_train[y_train==1,1], color='red', label='Class 1 (train)')\n",
        "plt.scatter(X_test[y_test==0,0], X_test[y_test==0,1], color='cyan', marker='x', label='Class 0 (test)')\n",
        "plt.scatter(X_test[y_test==1,0], X_test[y_test==1,1], color='orange', marker='x', label='Class 1 (test)')\n",
        "\n",
        "# Plot SVM decision boundary and margins\n",
        "ax = plt.gca()\n",
        "xlim = ax.get_xlim()\n",
        "ylim = ax.get_ylim()\n",
        "xx = np.linspace(xlim[0], xlim[1], 30)\n",
        "yy = np.linspace(ylim[0], ylim[1], 30)\n",
        "YY, XX = np.meshgrid(yy, xx)\n",
        "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
        "Z = svm.decision_function(xy).reshape(XX.shape)\n",
        "plt.contour(XX, YY, Z, colors='k', levels=[0], alpha=0.8, linestyles=['--'], linewidths=2, label='Decision boundary')\n",
        "plt.contour(XX, YY, Z, colors='grey', levels=[-1, 1], alpha=0.5, linestyles=[':'], linewidths=2)\n",
        "# Plot support vectors\n",
        "plt.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1], s=150, linewidth=2, facecolors='none', edgecolors='k', label='Support Vectors')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Simple SVM Example')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### user input"
      ],
      "metadata": {
        "id": "yZDC3g4hOCGG"
      },
      "id": "yZDC3g4hOCGG"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- User input and prediction ---\n",
        "try:\n",
        "    f1 = float(input('Enter value for feature1: '))\n",
        "    f2 = float(input('Enter value for feature2: '))\n",
        "    user_point = np.array([[f1, f2]])\n",
        "    user_pred = svm.predict(user_point)[0]\n",
        "    print(f'Predicted class for input ({f1}, {f2}): {user_pred}')\n",
        "    # Plot again with user point\n",
        "    plt.figure(figsize=(7,5))\n",
        "    plt.scatter(X_train[y_train==0,0], X_train[y_train==0,1], color='blue', label='Class 0 (train)')\n",
        "    plt.scatter(X_train[y_train==1,0], X_train[y_train==1,1], color='red', label='Class 1 (train)')\n",
        "    plt.scatter(X_test[y_test==0,0], X_test[y_test==0,1], color='cyan', marker='x', label='Class 0 (test)')\n",
        "    plt.scatter(X_test[y_test==1,0], X_test[y_test==1,1], color='orange', marker='x', label='Class 1 (test)')\n",
        "    # Decision boundary and margins\n",
        "    ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
        "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
        "    YY, XX = np.meshgrid(yy, xx)\n",
        "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
        "    Z = svm.decision_function(xy).reshape(XX.shape)\n",
        "    plt.contour(XX, YY, Z, colors='k', levels=[0], alpha=0.8, linestyles=['--'], linewidths=2)\n",
        "    plt.contour(XX, YY, Z, colors='grey', levels=[-1, 1], alpha=0.5, linestyles=[':'], linewidths=2)\n",
        "    plt.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1], s=150, linewidth=2, facecolors='none', edgecolors='k', label='Support Vectors')\n",
        "    # Plot user input\n",
        "    plt.scatter(f1, f2, color='magenta', s=200, marker='*', label='User Input')\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title('SVM with User Input')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(f'User input prediction skipped: {e}')"
      ],
      "metadata": {
        "id": "x04m3qCJODfg"
      },
      "id": "x04m3qCJODfg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PRINCIPAL COMPONENT ANALYSIS (PCA)"
      ],
      "metadata": {
        "id": "30ILctJKQQ8R"
      },
      "id": "30ILctJKQQ8R"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create a simple 2D dataset\n",
        "X = np.array([[2, 8], [3, 7], [4, 6], [5, 5], [6, 4], [7, 3], [8, 2]])\n",
        "\n",
        "# Fit PCA to reduce to 1 principal component\n",
        "pca = PCA(n_components=1)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "print(\"Original Data:\\n\", X)\n",
        "print(\"Transformed Data (1D):\\n\", X_pca)\n",
        "\n",
        "# Inverse transform to get back to 2D (approximate)\n",
        "X_inv = pca.inverse_transform(X_pca)\n",
        "print(\"Reconstructed Data (from 1D):\\n\", X_inv)\n",
        "\n",
        "# Plot original and reconstructed data side by side for comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Original data\n",
        "axes[0].scatter(X[:, 0], X[:, 1], color='blue', label='Original Data')\n",
        "axes[0].set_title('Original Data')\n",
        "axes[0].set_xlabel('Feature 1')\n",
        "axes[0].set_ylabel('Feature 2')\n",
        "axes[0].legend()\n",
        "\n",
        "# Reconstructed data (from 1D)\n",
        "axes[1].scatter(X_inv[:, 0], X_inv[:, 1], color='red', marker='x', label='Reconstructed (from 1D)')\n",
        "axes[1].set_title('Reconstructed Data (from 1D PCA)')\n",
        "axes[1].set_xlabel('Feature 1')\n",
        "axes[1].set_ylabel('Feature 2')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.suptitle('PCA Demonstration: Original vs Reconstructed Data')\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_soSaTjGQUsI"
      },
      "id": "_soSaTjGQUsI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes\n"
      ],
      "metadata": {
        "id": "uWUz7D2fhP1Q"
      },
      "id": "uWUz7D2fhP1Q"
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“¥ Load dataset from OpenML\n",
        "from sklearn.datasets import fetch_openml\n",
        "import pandas as pd\n",
        "\n",
        "df = fetch_openml(\"adult\", version=2, as_frame=True).frame\n",
        "df.head()"
      ],
      "metadata": {
        "id": "btMzLGMihU5Y"
      },
      "id": "btMzLGMihU5Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Drop missing values (represented by '?')\n",
        "df = df.replace('?', pd.NA).dropna()"
      ],
      "metadata": {
        "id": "kZU4K8DqhW_Z"
      },
      "id": "kZU4K8DqhW_Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸŽ¯ Encode categorical features\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "for col in df.select_dtypes(include=\"category\").columns:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# Split features and target\n",
        "X = df.drop(\"class\", axis=1)\n",
        "y = df[\"class\"]"
      ],
      "metadata": {
        "id": "TgR5RBHEhYT4"
      },
      "id": "TgR5RBHEhYT4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”€ Train/test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "kwfWI1dkhZ3I"
      },
      "id": "kwfWI1dkhZ3I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ¤– Gaussian Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ðŸ“Š Predictions\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "-2vwRhLEhb4x"
      },
      "id": "-2vwRhLEhb4x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“ˆ Evaluation\n",
        "print(\"âœ… Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nðŸ“‹ Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "gYyyDAQGhdWx"
      },
      "id": "gYyyDAQGhdWx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a few actual vs predicted values\n",
        "output_df = pd.DataFrame({\n",
        "    \"Actual\": y_test.values,\n",
        "    \"Predicted\": y_pred\n",
        "})\n",
        "\n",
        "# Show first 10 predictions\n",
        "print(\"\\nðŸ”® Sample Predictions:\\n\")\n",
        "print(output_df.head(10))\n"
      ],
      "metadata": {
        "id": "SUtMEn4Th7kH"
      },
      "id": "SUtMEn4Th7kH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Classifier"
      ],
      "metadata": {
        "id": "VmXqRSbx1IDa"
      },
      "id": "VmXqRSbx1IDa"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix,\n",
        "    accuracy_score, roc_curve, auc\n",
        ")\n",
        "\n",
        "#  Load dataset\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
        "df = pd.read_csv(url, sep=';')\n",
        "\n",
        "#  Binary classification: Good (>=6), Bad (<6)\n",
        "df['quality_label'] = df['quality'].apply(lambda q: 1 if q >= 6 else 0)\n",
        "X = df.drop(['quality', 'quality_label'], axis=1)\n",
        "y = df['quality_label']\n",
        "\n",
        "#  Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "#  Random Forest with class balancing\n",
        "model = RandomForestClassifier(n_estimators=150, max_depth=12, class_weight='balanced', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "#  Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=[\"Bad Wine\", \"Good Wine\"]))\n",
        "\n",
        "#  Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Bad\", \"Good\"], yticklabels=[\"Bad\", \"Good\"])\n",
        "plt.title(\"Confusion Matrix - Wine Quality (Binary)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#  ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})', color='darkorange')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='navy')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve - Wine Quality Classifier\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#  Feature Importance\n",
        "feat_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': model.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=True)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='Importance', y='Feature', data=feat_importance, palette='viridis')\n",
        "plt.title(\"Feature Importance (Random Forest)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#  Class Balance Check\n",
        "sns.countplot(x=y, palette='Set2')\n",
        "plt.title(\"Good vs Bad Wine Class Distribution\")\n",
        "plt.xlabel(\"Wine Class (0 = Bad, 1 = Good)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uZpOfVKu1QEw"
      },
      "id": "uZpOfVKu1QEw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K Means Clustering"
      ],
      "metadata": {
        "id": "UhCiLvBd8K7A"
      },
      "id": "UhCiLvBd8K7A"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the Iris dataset\n",
        "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "X = df.drop(\"species\", axis=1)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "inertia = []\n",
        "K_range = range(1, 7)\n",
        "for k in K_range:\n",
        "    km = KMeans(n_clusters=k, random_state=42)\n",
        "    km.fit(X_scaled)\n",
        "    inertia.append(km.inertia_)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(K_range, inertia, 'bo-')\n",
        "plt.xlabel(\"Number of Clusters (K)\")\n",
        "plt.ylabel(\"Inertia (Sum of Squared Distances)\")\n",
        "plt.title(\"Elbow Method for Iris Clustering\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# KMeans with K=3\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "clusters = kmeans.fit_predict(X_scaled)\n",
        "df['Cluster'] = clusters\n",
        "\n",
        "# Visualize clusters with PCA in 2D\n",
        "pca = PCA(n_components=2)\n",
        "components = pca.fit_transform(X_scaled)\n",
        "df['PC1'], df['PC2'] = components[:,0], components[:,1]\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(data=df, x='PC1', y='PC2', hue='Cluster', palette='Set1', s=100, alpha=0.8)\n",
        "plt.title(\"K-Means Clusters on Iris (PCA projection)\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend(title='Cluster')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare clusters with true species\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(data=df, x='PC1', y='PC2', hue='species', palette='Set2', s=100, alpha=0.8)\n",
        "plt.title(\"Actual Iris Species (PCA projection)\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.legend(title='Species')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Cluster centers in original feature space\n",
        "centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
        "center_df = pd.DataFrame(centers, columns=X.columns).round(2)\n",
        "print(\"Cluster Centers (unscaled feature averages):\")\n",
        "print(center_df)\n",
        "\n",
        "# Cluster composition summary\n",
        "print(\"\\nCluster Composition:\")\n",
        "print(df.groupby(['Cluster', 'species']).size().unstack(fill_value=0))\n"
      ],
      "metadata": {
        "id": "nIC1KTQ-8SLw"
      },
      "id": "nIC1KTQ-8SLw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}